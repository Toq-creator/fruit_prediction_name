# -*- coding: utf-8 -*-
"""Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WQBj2wKET05pkG4dj2lv9L5hs1UD-k5j
"""

# ============================================
# STEP 1: Import Libraries and Load Dataset
# ============================================
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import joblib

# Load dataset (upload fruit_classification_dataset.csv to Colab first)
df = pd.read_csv("/content/fruit_classification_dataset.csv")

print("Dataset shape:", df.shape)
display(df.head())

# ============================================
# STEP 2: Preprocess Data
# ============================================

# Automatically detect target column (assumed to be the last one)
target_col = df.columns[-1]
print("Target column:", target_col)

# Encode target labels if categorical
le = None
if df[target_col].dtype == 'object':
    le = LabelEncoder()
    df[target_col] = le.fit_transform(df[target_col])

# Split features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# Identify categorical columns in X
categorical_cols = X.select_dtypes(include='object').columns

# Apply one-hot encoding to categorical columns
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Scale numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# ============================================
# STEP 3: Build ANN Model
# ============================================
num_classes = len(pd.Series(y).unique())

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy',
    metrics=['accuracy']
)

# ============================================
# STEP 4: Train Model
# ============================================
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    verbose=1
)

# ============================================
# STEP 5: Evaluate Model - Confusion Matrix
# ============================================
y_pred = model.predict(X_test)

if num_classes > 2:
    y_pred_classes = y_pred.argmax(axis=1)
else:
    y_pred_classes = (y_pred > 0.5).astype("int32").flatten()

cm = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print("\nClassification Report:\n", classification_report(y_test, y_pred_classes))

# ============================================
# STEP 6: Save Model and Preprocessing Objects
# ============================================

# Save TensorFlow model
model.save("/content/fruit_classifier_model.h5")
print("‚úÖ Neural network model saved as fruit_classifier_model.h5")

# Save preprocessing tools
joblib.dump(scaler, "/content/scaler.joblib")
print("‚úÖ Scaler saved as scaler.joblib")

if le is not None:
    joblib.dump(le, "/content/label_encoder.joblib")
    print("‚úÖ LabelEncoder saved as label_encoder.joblib")
else:
    print("‚ÑπÔ∏è LabelEncoder not used (target was numeric).")

print("\nüéâ All files saved successfully!")

# ============================================
# STEP 7 (Optional): Example - Reload and Predict
# ============================================
# Example of how to load the model and preprocessing tools again:
"""
from tensorflow.keras.models import load_model
import joblib

# Load everything
model = load_model("/content/fruit_classifier_model.h5")
scaler = joblib.load("/content/scaler.joblib")
le = joblib.load("/content/label_encoder.joblib")

# Example new data (replace with real feature values)
new_data = pd.DataFrame([{
    'feature1': 5.6,
    'feature2': 3.1,
    'feature3': 1.4,
    'feature4': 0.2
}])

# Apply same encoding and scaling
new_data_encoded = pd.get_dummies(new_data)
new_data_encoded = new_data_encoded.reindex(columns=X_encoded.columns, fill_value=0)
new_data_scaled = scaler.transform(new_data_encoded)

# Predict
prediction = model.predict(new_data_scaled)
pred_class = prediction.argmax(axis=1)[0] if num_classes > 2 else int(prediction[0] > 0.5)

# Decode label (if LabelEncoder used)
if le is not None:
    pred_label = le.inverse_transform([pred_class])[0]
    print("Predicted Fruit Class:", pred_label)
else:
    print("Predicted Class:", pred_class)
"""

# ============================================
# STEP 1: Import Libraries and Load Dataset
# ============================================
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import joblib  # ‚úÖ Added for saving preprocessing tools

# Load dataset (upload fruit_classification_dataset.csv to Colab first)
df = pd.read_csv("/content/fruit_classification_dataset.csv")

print("Dataset shape:", df.shape)
df.head()

# ============================================
# STEP 2: Preprocess Data
# ============================================

# Automatically detect target column (assumed to be the last one)
target_col = df.columns[-1]
print("Target column:", target_col)

# Encode target labels if categorical
le = None
if df[target_col].dtype == 'object':
    le = LabelEncoder()
    df[target_col] = le.fit_transform(df[target_col])

# Split features and target
X = df.drop(columns=[target_col])
y = df[target_col]

# Identify categorical columns in X
categorical_cols = X.select_dtypes(include='object').columns

# Apply one-hot encoding to categorical columns
X_encoded = pd.get_dummies(X, columns=categorical_cols, drop_first=True)

# Scale numerical features (now all features are numerical after encoding)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_encoded)

# Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

# ============================================
# STEP 3: Build ANN Model
# ============================================
num_classes = len(pd.Series(y).unique())

model = Sequential([
    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')
])

model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy',
    metrics=['accuracy']
)

# ============================================
# STEP 4: Train Model
# ============================================
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=50,
    batch_size=32,
    verbose=1
)

# ============================================
# STEP 5: Evaluate Model - Confusion Matrix
# ============================================
y_pred = model.predict(X_test)

if num_classes > 2:
    y_pred_classes = y_pred.argmax(axis=1)
else:
    y_pred_classes = (y_pred > 0.5).astype("int32").flatten()

cm = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print("\nClassification Report:\n", classification_report(y_test, y_pred_classes))

# ============================================
# ‚úÖ STEP 6: Save Preprocessing Tools with joblib
# ============================================
joblib.dump(scaler, "/content/scaler.joblib")
print("‚úÖ Scaler saved as scaler.joblib")

if le is not None:
    joblib.dump(le, "/content/label_encoder.joblib")
    print("‚úÖ LabelEncoder saved as label_encoder.joblib")
else:
    print("‚ÑπÔ∏è LabelEncoder not used (target was numeric).")